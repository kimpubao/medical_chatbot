{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436c4749",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct 모델_End-to-End 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a583c6",
   "metadata": {},
   "source": [
    "## \"학습” 안 하는 이유\n",
    "1. 데이터 로딩/전처리:\n",
    "의료 QA 데이터 등 → 텍스트 조각(Document)으로 분할 및 필터링\n",
    "2. 임베딩:\n",
    "Ko-SBERT 등 “사전학습된 임베딩 모델”로 텍스트를 벡터로 변환\n",
    "(여기서 학습이 아니라 “이미 학습된 모델로 변환”만 하는 것)\n",
    "3. 벡터DB 생성:\n",
    "FAISS에 임베딩 벡터 저장\n",
    "4. 질의응답:\n",
    "사용자가 질문하면 →\n",
    "    - (1) 벡터DB에서 유사한 문서 검색\n",
    "    - (2) Mistral-7B-Instruct 등 대형 언어모델(역시 “사전학습된”)이\n",
    "검색된 문서 조각들을 참고해서 답변 생성\n",
    "## 실제 “학습”이란?\n",
    "- 파인튜닝/미세조정 등은\n",
    "모델을 직접 추가로 훈련시키는 것 (모델 파라미터가 변함)\n",
    "- 예시: model.fit(), trainer.train(), “epochs”, “loss” 등이 코드에 나옴\n",
    "\n",
    "지금 코드의 RAG 구조는:\n",
    "- 임베딩 모델(KoSBERT): 이미 학습된 걸 “불러와서” 임베딩만 실행\n",
    "- LLM(Mistral-7B-Instruct 등): 이미 학습된 걸 “불러와서” 답변 생성만 하는 것\n",
    "- 파인튜닝(학습) 없음\n",
    "\n",
    "## 요약\n",
    "“임베딩/벡터DB/질의응답 파이프라인”이지\n",
    "모델 파인튜닝(학습)은 안 하는 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2e138",
   "metadata": {},
   "source": [
    "# “RAG 없는 LLM 챗봇”\n",
    "1. 학습(파인튜닝)이 필요한 경우가 많음\n",
    "2. LLM(예: Mistral, Llama, GPT 등)만 놓고 사용하면\n",
    "- → 사전학습 범위 내의 지식만 답변\n",
    "- → 회사/기관/도메인 고유 정보, 최신 정보 반영 불가\n",
    "- → 답변 품질, 일관성 낮음\n",
    "\n",
    "3. **우리 데이터/지식으로 LLM을 ‘맞춤화’**하고 싶으면\n",
    "→ 파인튜닝(학습)이 필요\n",
    "\n",
    "# “RAG 있는 LLM 챗봇”\n",
    "1. 학습(파인튜닝) 없이도 실제 업무·실전 서비스 가능\n",
    "- 내·외부 문서, 자료를 벡터DB로 embedding해서\n",
    "- “검색-답변(Retrieval + Generation)” 방식으로 동작\n",
    "- LLM은 문서에서 답을 찾아서 생성\n",
    "- 우리 데이터만 잘 embedding하면 바로 적용\n",
    "(LLM은 사전학습된 상태 그대로 사용)\n",
    "\n",
    "# 정리\n",
    "1. RAG 없음 → 파인튜닝 거의 필수\n",
    "(실무 적용/특화된 답변 원할 때)\n",
    "\n",
    "2. RAG 있음 → 파인튜닝 필요 없음\n",
    "(거의 모든 도메인 챗봇, 사내 QA 가능)\n",
    "\n",
    "# RAG는 “검색+생성” 구조라 우리 데이터를 LLM에 직접 넣어주지 않아도, 최신/도메인 정보로 답변 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de748c41",
   "metadata": {},
   "source": [
    "===================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA 데이터셋 구조 확인\n",
    "import os, json\n",
    "\n",
    "label_dirs = [\n",
    "    \"dataset/medical_knowledge_QA/Training/02.라벨링데이터\",\n",
    "    \"dataset/medical_knowledge_QA/Validation/02.라벨링데이터\",\n",
    "]\n",
    "\n",
    "e2e_train = []\n",
    "for label_dir in label_dirs:\n",
    "    for filename in os.listdir(label_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(label_dir, filename)\n",
    "            with open(file_path, encoding=\"utf-8-sig\") as f:\n",
    "                data = json.load(f)\n",
    "                # 데이터 구조: data[\"question\"], data[\"answer\"], data[\"context\"] (context 없으면 \"\" 또는 없음)\n",
    "                question = data.get(\"question\", \"\")\n",
    "                answer = data.get(\"answer\", \"\")\n",
    "                context = data.get(\"context\", \"\")\n",
    "                if question and answer:\n",
    "                    e2e_train.append(\n",
    "                        {\n",
    "                            \"question\": question,\n",
    "                            \"context\": context,\n",
    "                            \"answer\": answer,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "print(\"총 QA 샘플 수:\", len(e2e_train))\n",
    "print(\"첫 번째 샘플:\", e2e_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19572b5",
   "metadata": {},
   "source": [
    "## 임포트 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e970cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3a882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "# GPU 확인용 코드\n",
    "# CPU 사용시 느림 현상으로 멈춤 => GPU 사용\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def27dc2",
   "metadata": {},
   "source": [
    "## 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012e60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 네이버 JSON\n",
    "with open(\"dataset/naver_docs.json\", encoding=\"utf-8\") as f:\n",
    "    naver = json.load(f)\n",
    "# 2) AMC JSON\n",
    "with open(\"dataset/amc_docs.json\", encoding=\"utf-8\") as f:\n",
    "    amc = json.load(f)\n",
    "# 3) medical_knowledge_QA\n",
    "label_dirs = [\n",
    "    \"dataset/medical_knowledge_QA/Training/02.라벨링데이터\",\n",
    "    \"dataset/medical_knowledge_QA/Validation/02.라벨링데이터\",\n",
    "]\n",
    "all_qa_data = []\n",
    "for label_dir in label_dirs:\n",
    "    for filename in os.listdir(label_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(label_dir, filename)\n",
    "            with open(file_path, encoding=\"utf-8-sig\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    all_qa_data.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"[오류] {filename}: {e}\")\n",
    "\n",
    "# 4) medical_legal_corpus 폴더\n",
    "text_dir = \"dataset/medical_legal_corpus/Training/01.원천데이터\"\n",
    "all_texts = []\n",
    "for filename in os.listdir(text_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(text_dir, filename)\n",
    "        with open(file_path, encoding=\"utf-8-sig\") as f:\n",
    "            text = f.read()\n",
    "            all_texts.append({\"filename\": filename, \"text\": text})\n",
    "\n",
    "# 5) 라벨링 QA (End-to-End 파인튜닝 데이터 준비)\n",
    "label_path = (\n",
    "    \"dataset/medical_legal_corpus/Training/02.라벨링데이터/Training_medical.json\"\n",
    ")\n",
    "with open(label_path, encoding=\"utf-8-sig\") as f:\n",
    "    legal_qa_data = json.load(f)\n",
    "qa_items = legal_qa_data.get(\"data\", [])\n",
    "e2e_train = []\n",
    "for qa in qa_items:\n",
    "    question = qa.get(\"question\", \"\")\n",
    "    context = qa.get(\"context\", \"\")\n",
    "    answer = qa.get(\"answer\", \"\")\n",
    "    if question and context and answer:\n",
    "        e2e_train.append({\"question\": question, \"context\": context, \"answer\": answer})\n",
    "\n",
    "# 6) PDF → 텍스트 추출 및 분할 (필요시)\n",
    "pdf_path = \"dataset/antibiotic_guideline_for_longtermcare.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "all_text = \"\"\n",
    "for page in reader.pages:\n",
    "    all_text += page.extract_text() + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a559e",
   "metadata": {},
   "source": [
    "## 3. 텍스트 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea0c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "# 네이버, 아산병원\n",
    "texts += [doc[\"text\"] for doc in naver]\n",
    "texts += [doc[\"text\"] for doc in amc]\n",
    "# medical_knowledge_QA\n",
    "for item in all_qa_data:\n",
    "    if isinstance(item, list):\n",
    "        for qa in item:\n",
    "            q = qa.get(\"question\", \"\")\n",
    "            a = qa.get(\"answer\", \"\")\n",
    "            if q or a:\n",
    "                texts.append(q + \"\\n\" + a)\n",
    "# medical_legal_corpus\n",
    "texts += [doc[\"text\"] for doc in all_texts if isinstance(doc, dict) and \"text\" in doc]\n",
    "# 라벨링 QA\n",
    "texts += [qa.get(\"text\", \"\") for qa in qa_items if \"text\" in qa]\n",
    "# PDF\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_text(all_text)\n",
    "texts += chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e86db",
   "metadata": {},
   "source": [
    "## 4. 텍스트 분할 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da450d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 문서 수: 308545\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "documents = splitter.create_documents(texts)\n",
    "\n",
    "# 전처리(예: 너무 짧거나, 유전자서열/스팸/광고 제거, 키워드 필터링)\n",
    "MEDICAL_KEYWORDS = [\n",
    "    \"병원\",\n",
    "    \"의과\",\n",
    "    \"내과\",\n",
    "    \"이비인후과\",\n",
    "    \"피부과\",\n",
    "    \"정형외과\",\n",
    "    \"신경과\",\n",
    "    \"정신과\",\n",
    "    \"산부인과\",\n",
    "    \"응급실\",\n",
    "    \"진료\",\n",
    "    \"의사\",\n",
    "    \"간호사\",\n",
    "    \"환자\",\n",
    "    \"보호자\",\n",
    "    \"수술\",\n",
    "    \"진단\",\n",
    "    \"시술\",\n",
    "    \"검사\",\n",
    "    \"CT\",\n",
    "    \"MRI\",\n",
    "    \"X-ray\",\n",
    "    \"혈액\",\n",
    "    \"소변\",\n",
    "    \"조영제\",\n",
    "    \"항생제\",\n",
    "    \"주사\",\n",
    "    \"투약\",\n",
    "    \"약물\",\n",
    "    \"처방\",\n",
    "    \"상담\",\n",
    "    \"증상\",\n",
    "    \"발열\",\n",
    "    \"기침\",\n",
    "    \"두통\",\n",
    "    \"감기\",\n",
    "    \"인후통\",\n",
    "    \"복통\",\n",
    "    \"설사\",\n",
    "    \"요로감염\",\n",
    "    \"폐렴\",\n",
    "    \"당뇨\",\n",
    "    \"고혈압\",\n",
    "    \"암\",\n",
    "    \"코로나\",\n",
    "    \"독감\",\n",
    "    \"백신\",\n",
    "    \"면역\",\n",
    "    \"질환\",\n",
    "    \"질병\",\n",
    "    \"감염\",\n",
    "    \"감염병\",\n",
    "    \"폐렴\",\n",
    "    \"요로감염\",\n",
    "    \"피부염\",\n",
    "    \"욕창\",\n",
    "    \"MRSA\",\n",
    "    \"바이러스\",\n",
    "]\n",
    "clean_documents = []\n",
    "for doc in documents:\n",
    "    text = doc.page_content.strip()\n",
    "    if len(text) < 10:\n",
    "        continue\n",
    "    if re.fullmatch(r\"[ACGT\\n]+\", text):\n",
    "        continue\n",
    "    if any(word in text for word in MEDICAL_KEYWORDS):\n",
    "        clean_documents.append(doc)\n",
    "documents = clean_documents\n",
    "print(f\"전처리 후 문서 수: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb92f1",
   "metadata": {},
   "source": [
    "## 5. KoSBERT 임베딩 및 FAISS 벡터DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff80f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23028\\3877256484.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sbert-sts\")\n",
      "c:\\Users\\user\\Desktop\\medical_chatbot\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 벡터 DB 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sbert-sts\")\n",
    "db = FAISS.from_documents(documents, embedding=embedding_model)\n",
    "db.save_local(\"faiss_medical_knowledge\")\n",
    "print(\"FAISS 벡터 DB 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd37e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='가랑이통증 [Perineal pain]은(는) 원인에 따라 증상에 다소 차이가 있을 수 있다. *요로감염:  배뇨통  및 빈뇨,  절박뇨 , 골반부위의 둔통 등 *항문주위농양: 발적과 종창, 압통 및 욱신거리는 통증을 동반한 덩이(감염에 의한 경우 열 및 오한을 동반할 수 있음) *음부신경포착증후군: 앉은자세일때 심해지는 통증 *골반바닥근육의 이상: 배뇨, 배변시의 장애 동반 등 증상이 나타나는 질환이며, 관련 진료과는 산부인과입니다.'), Document(metadata={}, page_content='가성근시 [pseudomyopia]은(는) 원거리 시력저하가 나타나고 과도한 모양체 근육의 수축으로 인해서 눈이 피로하고 안구통증, 두통, 어지러움 등이 나타날 수 있다. 증상이 나타나는 질환이며, 관련 진료과는 안과, 정신건강의학과입니다.'), Document(metadata={}, page_content='가스 괴저병 [gas gangrene]은(는) 가스 괴저병은 빠른 속도로 진행하며 생명을 위협할 수준까지 악화되기도 한다. 상처를 통해서 균이 침투한 후, 증상이 나타나기까지 걸리는 잠복기는 대개 1~4일 정도이다. 처음에는 상처 부위에 매우 심한 통증이 발생하고, 수분에서 수시간 사이에 빠르게 진행하여 감염 부위가 붓고 창백하게 변하며 누를 때 통증(동통)이 발생한다. 연부조직 부위에 차오르는 가스는 방사선 사진을 통해 눈으로 확인할 수 있고, 또는 손으로 눌러보면 가스가 발생하는 것을 알 수 있다. 가스 괴저병이 발생한 부위의 피부는 초기의 창백한 색조에서 점점 색깔이 변하여 구릿빛으로 진해지며, 때때로 출혈이 동반된 물집이 잡히기도 한다.  식은땀 이 나고 심장 박동수가 빨라지며 심하게 안절부절 못하는 등의 전신적인 증상을 동반하기도 한다. 조기에 진단하여 치료를 하지 않으면 병이 발생한 팔이나 다리를 절제해야 하며, 이 시기를 놓치면 사망에 이를 수 있다. 증상이 나타나는 질환이며, 관련 진료과는 감염내과, 정형외과입니다.')]\n"
     ]
    }
   ],
   "source": [
    "print(documents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f832ef",
   "metadata": {},
   "source": [
    "## 6. LLM 로딩 및 QA 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0fcc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5217f9963684b87b233aa46d5af25d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23028\\3400874234.py:21: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "HF_TOKEN = \"your_token_here\"  # HuggingFace 토큰\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, token=HF_TOKEN, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    \"faiss_medical_knowledge\", embedding_model, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f70e8d",
   "metadata": {},
   "source": [
    "## 7. End-to-End 파인튜닝 데이터셋 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37d5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 33756, val: 3751\n"
     ]
    }
   ],
   "source": [
    "# (추가로 train/val 분리, ColBERT 등 학습코드로 바로 넘길 수 있음)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(qa_items, test_size=0.1, random_state=42)\n",
    "\n",
    "with open(\"medical_knowledge_QA_split.json\", \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    json.dump({\"train\": train, \"validation\": val}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"train: {len(train)}, val: {len(val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b754a1",
   "metadata": {},
   "source": [
    "## 8. 실시간 QA 인터페이스 예시(ipywidgets 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97babe",
   "metadata": {},
   "source": [
    "8-1 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e49c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fa83d67404402b98ee282eb0a9fa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='질문:', placeholder='여기에 질문을 입력하세요')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a174d42324e34ceeb047a1df3c182c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='질문하기', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abed4099b76947d5869dc787892017e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "question_box = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"여기에 질문을 입력하세요\",\n",
    "    description=\"질문:\",\n",
    "    disabled=False,\n",
    ")\n",
    "output_box = widgets.Output()\n",
    "submit_btn = widgets.Button(description=\"질문하기\")\n",
    "\n",
    "\n",
    "def on_submit(btn):\n",
    "    query = question_box.value\n",
    "    if query.strip().lower() == \"exit\":\n",
    "        with output_box:\n",
    "            clear_output()\n",
    "            print(\"종료합니다.\")\n",
    "        return\n",
    "    with output_box:\n",
    "        clear_output()\n",
    "        print(\"답변을 생성하고 있습니다...\")\n",
    "        result = qa_chain({\"query\": query})\n",
    "        print(\"\\n[답변]:\", result[\"result\"])\n",
    "        print(\"=\" * 80)\n",
    "        print(\"[참고 문서]\")\n",
    "        for i, doc in enumerate(result[\"source_documents\"]):\n",
    "            print(f\"\\n[{i+1}] {doc.page_content[:300]}...\")\n",
    "\n",
    "\n",
    "submit_btn.on_click(on_submit)\n",
    "display(question_box, submit_btn, output_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f2c4f",
   "metadata": {},
   "source": [
    "8-2 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb14ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cd81590f2348dc9af0586d0260e462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='질문:', placeholder='여기에 질문을 입력하세요')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d85f8b1d7d44e29cf9da60fb17a2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='질문하기', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567f03528a1c4e5db7058bc8df3a273e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "question_box = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"여기에 질문을 입력하세요\",\n",
    "    description=\"질문:\",\n",
    "    disabled=False,\n",
    ")\n",
    "output_box = widgets.Output()\n",
    "submit_btn = widgets.Button(description=\"질문하기\")\n",
    "\n",
    "\n",
    "def on_submit(btn):\n",
    "    query = question_box.value\n",
    "    with output_box:\n",
    "        clear_output()\n",
    "        if not query.strip():\n",
    "            print(\"질문을 입력해 주세요!\")\n",
    "            return\n",
    "        if query.strip().lower() == \"exit\":\n",
    "            print(\"종료합니다.\")\n",
    "            return\n",
    "        print(\"답변을 생성하고 있습니다...\")\n",
    "        try:\n",
    "            result = qa_chain({\"query\": query})\n",
    "            print(\n",
    "                \"\\n[답변]:\", result.get(\"result\", \"응답이 없습니다.\")[:1000]\n",
    "            )  # 답변 1000자 제한 예시\n",
    "            print(\"=\" * 80)\n",
    "            print(\"[참고 문서]\")\n",
    "            for i, doc in enumerate(\n",
    "                result.get(\"source_documents\", [])[:3]\n",
    "            ):  # 3개까지만 미리보기\n",
    "                print(f\"\\n[{i+1}] {doc.page_content[:500]}...\")\n",
    "        except Exception as e:\n",
    "            print(\"답변 생성 중 오류 발생:\", e)\n",
    "\n",
    "\n",
    "submit_btn.on_click(on_submit)\n",
    "display(question_box, submit_btn, output_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b043a51",
   "metadata": {},
   "source": [
    "# 현재 코드와 End-to-End 활용성\n",
    "\n",
    "## 1. End-to-End 파이프라인이란?\n",
    "\n",
    "- **질문 → 관련 문서 검색 → 답변 생성**\n",
    "- 이 전체 과정을 한 번에 자동화한 것이 End-to-End 방식\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 현재 코드 흐름\n",
    "\n",
    "### 1) 데이터 전처리 및 임베딩\n",
    "\n",
    "- 여러 소스에서 데이터를 수집하고 전처리(필터링) 후,\n",
    "- `KoSBERT` 임베딩 → **FAISS 벡터DB에 저장**\n",
    "    - 이 과정이 검색(검색 DB 구축) 역할을 함\n",
    "\n",
    "### 2) 질문 입력 & QA 체인 연결\n",
    "\n",
    "- 사용자가 `ipywidgets`로 질문을 입력하면,\n",
    "- **RAG 파이프라인**이\n",
    "    1. 질문을 임베딩하여 FAISS에서 **관련 문서(컨텍스트) 검색**\n",
    "    2. 검색된 문서들을 LLM(예: Mistral-7B)에 넣어서 **최종 답변 생성**\n",
    "- 사용자는 중간 과정 없이 곧바로 답변을 받음  \n",
    "    → **이게 바로 End-to-End 흐름**\n",
    "\n",
    "### 3) End-to-End 데이터셋 분리\n",
    "\n",
    "- 별도로 준비한 End-to-End 학습용 데이터셋  \n",
    "    (질문/문서/답변 페어)\n",
    "- ColBERT 등 파인튜닝, 성능 평가, 추가 실험에도 바로 사용 가능\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 활용 관점\n",
    "\n",
    "- **실시간 질문-답변**  \n",
    "    중간 수동 개입 없이, 사용자가 질문만 하면  \n",
    "    “검색 → 답변 생성”까지 바로 진행\n",
    "\n",
    "- **파인튜닝 데이터**  \n",
    "    End-to-End 구조로 학습 데이터를 만들어  \n",
    "    LLM 추가 개선, 평가 등도 쉽게 진행 가능\n",
    "\n",
    "---\n",
    "\n",
    "## 결론\n",
    "\n",
    "- **End-to-End RAG 구조가 전체 파이프라인에서 자연스럽게 동작하고 있습니다.**\n",
    "- 현재 코드는 RAG 구조(검색+생성)와 End-to-End 철학에 모두 부합합니다.\n",
    "- 실제 서비스, 프로토타입, 실험, 파인튜닝, 응용 등 다양한 곳에서  \n",
    "  **효과적으로 활용할 수 있습니다!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b7eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
