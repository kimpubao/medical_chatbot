{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.7257844474761255,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.027285129604365622,
      "grad_norm": 19.279239654541016,
      "learning_rate": 0.00019963669391462308,
      "loss": 6.0411,
      "step": 10
    },
    {
      "epoch": 0.054570259208731244,
      "grad_norm": 2.5528886318206787,
      "learning_rate": 0.00019782016348773843,
      "loss": 3.6386,
      "step": 20
    },
    {
      "epoch": 0.08185538881309687,
      "grad_norm": 2.525735855102539,
      "learning_rate": 0.00019600363306085378,
      "loss": 3.0048,
      "step": 30
    },
    {
      "epoch": 0.10914051841746249,
      "grad_norm": 2.40120792388916,
      "learning_rate": 0.00019418710263396913,
      "loss": 2.534,
      "step": 40
    },
    {
      "epoch": 0.1364256480218281,
      "grad_norm": 2.6669492721557617,
      "learning_rate": 0.00019237057220708448,
      "loss": 2.4956,
      "step": 50
    },
    {
      "epoch": 0.16371077762619374,
      "grad_norm": 3.652996778488159,
      "learning_rate": 0.00019055404178019983,
      "loss": 2.4041,
      "step": 60
    },
    {
      "epoch": 0.19099590723055934,
      "grad_norm": 3.5828914642333984,
      "learning_rate": 0.00018873751135331518,
      "loss": 2.3814,
      "step": 70
    },
    {
      "epoch": 0.21828103683492497,
      "grad_norm": 2.646312952041626,
      "learning_rate": 0.00018692098092643053,
      "loss": 2.3391,
      "step": 80
    },
    {
      "epoch": 0.24556616643929058,
      "grad_norm": 2.5306460857391357,
      "learning_rate": 0.00018510445049954588,
      "loss": 2.384,
      "step": 90
    },
    {
      "epoch": 0.2728512960436562,
      "grad_norm": 3.784843921661377,
      "learning_rate": 0.00018328792007266123,
      "loss": 2.2825,
      "step": 100
    },
    {
      "epoch": 0.30013642564802184,
      "grad_norm": 2.6867058277130127,
      "learning_rate": 0.00018147138964577658,
      "loss": 2.2511,
      "step": 110
    },
    {
      "epoch": 0.3274215552523875,
      "grad_norm": 2.441169023513794,
      "learning_rate": 0.00017965485921889193,
      "loss": 2.2726,
      "step": 120
    },
    {
      "epoch": 0.35470668485675305,
      "grad_norm": 2.3958828449249268,
      "learning_rate": 0.00017783832879200728,
      "loss": 2.2245,
      "step": 130
    },
    {
      "epoch": 0.3819918144611187,
      "grad_norm": 2.2291574478149414,
      "learning_rate": 0.00017602179836512263,
      "loss": 2.2548,
      "step": 140
    },
    {
      "epoch": 0.4092769440654843,
      "grad_norm": 3.1530942916870117,
      "learning_rate": 0.00017420526793823798,
      "loss": 2.2999,
      "step": 150
    },
    {
      "epoch": 0.43656207366984995,
      "grad_norm": 2.682671070098877,
      "learning_rate": 0.0001723887375113533,
      "loss": 2.2632,
      "step": 160
    },
    {
      "epoch": 0.4638472032742155,
      "grad_norm": 2.2810769081115723,
      "learning_rate": 0.00017057220708446868,
      "loss": 2.2461,
      "step": 170
    },
    {
      "epoch": 0.49113233287858116,
      "grad_norm": 2.312946319580078,
      "learning_rate": 0.00016875567665758403,
      "loss": 2.1867,
      "step": 180
    },
    {
      "epoch": 0.5184174624829468,
      "grad_norm": 2.678558349609375,
      "learning_rate": 0.00016693914623069938,
      "loss": 2.1794,
      "step": 190
    },
    {
      "epoch": 0.5457025920873124,
      "grad_norm": 2.783120632171631,
      "learning_rate": 0.00016512261580381473,
      "loss": 2.1509,
      "step": 200
    },
    {
      "epoch": 0.572987721691678,
      "grad_norm": 3.313204050064087,
      "learning_rate": 0.00016330608537693005,
      "loss": 2.2433,
      "step": 210
    },
    {
      "epoch": 0.6002728512960437,
      "grad_norm": 3.4513309001922607,
      "learning_rate": 0.00016148955495004543,
      "loss": 2.247,
      "step": 220
    },
    {
      "epoch": 0.6275579809004093,
      "grad_norm": 2.3688673973083496,
      "learning_rate": 0.00015967302452316078,
      "loss": 2.173,
      "step": 230
    },
    {
      "epoch": 0.654843110504775,
      "grad_norm": 5.044154644012451,
      "learning_rate": 0.00015785649409627613,
      "loss": 2.1987,
      "step": 240
    },
    {
      "epoch": 0.6821282401091405,
      "grad_norm": 3.622396230697632,
      "learning_rate": 0.00015603996366939145,
      "loss": 2.2211,
      "step": 250
    },
    {
      "epoch": 0.7094133697135061,
      "grad_norm": 4.065269470214844,
      "learning_rate": 0.0001542234332425068,
      "loss": 2.1063,
      "step": 260
    },
    {
      "epoch": 0.7366984993178718,
      "grad_norm": 2.7006874084472656,
      "learning_rate": 0.00015240690281562218,
      "loss": 2.178,
      "step": 270
    },
    {
      "epoch": 0.7639836289222374,
      "grad_norm": 2.943894147872925,
      "learning_rate": 0.00015059037238873753,
      "loss": 2.1818,
      "step": 280
    },
    {
      "epoch": 0.791268758526603,
      "grad_norm": 2.364982843399048,
      "learning_rate": 0.00014877384196185288,
      "loss": 2.1723,
      "step": 290
    },
    {
      "epoch": 0.8185538881309686,
      "grad_norm": 2.5370309352874756,
      "learning_rate": 0.0001469573115349682,
      "loss": 2.1207,
      "step": 300
    },
    {
      "epoch": 0.8458390177353342,
      "grad_norm": 2.1104509830474854,
      "learning_rate": 0.00014514078110808355,
      "loss": 2.0352,
      "step": 310
    },
    {
      "epoch": 0.8731241473396999,
      "grad_norm": 3.019289255142212,
      "learning_rate": 0.00014332425068119893,
      "loss": 2.1133,
      "step": 320
    },
    {
      "epoch": 0.9004092769440655,
      "grad_norm": 3.2557590007781982,
      "learning_rate": 0.00014150772025431428,
      "loss": 2.1409,
      "step": 330
    },
    {
      "epoch": 0.927694406548431,
      "grad_norm": 3.04079008102417,
      "learning_rate": 0.0001396911898274296,
      "loss": 2.0845,
      "step": 340
    },
    {
      "epoch": 0.9549795361527967,
      "grad_norm": 2.3956639766693115,
      "learning_rate": 0.00013787465940054495,
      "loss": 2.0689,
      "step": 350
    },
    {
      "epoch": 0.9822646657571623,
      "grad_norm": 1.9758872985839844,
      "learning_rate": 0.00013605812897366033,
      "loss": 2.1187,
      "step": 360
    },
    {
      "epoch": 1.0081855388813097,
      "grad_norm": 2.907834529876709,
      "learning_rate": 0.00013424159854677568,
      "loss": 1.9826,
      "step": 370
    },
    {
      "epoch": 1.0354706684856754,
      "grad_norm": 2.0962696075439453,
      "learning_rate": 0.00013242506811989103,
      "loss": 1.9744,
      "step": 380
    },
    {
      "epoch": 1.0627557980900408,
      "grad_norm": 2.7414300441741943,
      "learning_rate": 0.00013060853769300635,
      "loss": 1.8958,
      "step": 390
    },
    {
      "epoch": 1.0900409276944065,
      "grad_norm": 3.1482019424438477,
      "learning_rate": 0.0001287920072661217,
      "loss": 2.044,
      "step": 400
    },
    {
      "epoch": 1.1173260572987722,
      "grad_norm": 2.683978319168091,
      "learning_rate": 0.00012697547683923708,
      "loss": 1.9862,
      "step": 410
    },
    {
      "epoch": 1.1446111869031377,
      "grad_norm": 4.148189544677734,
      "learning_rate": 0.00012515894641235243,
      "loss": 1.9,
      "step": 420
    },
    {
      "epoch": 1.1718963165075034,
      "grad_norm": 3.0786330699920654,
      "learning_rate": 0.00012334241598546775,
      "loss": 2.0014,
      "step": 430
    },
    {
      "epoch": 1.199181446111869,
      "grad_norm": 2.663026809692383,
      "learning_rate": 0.00012152588555858311,
      "loss": 1.928,
      "step": 440
    },
    {
      "epoch": 1.2264665757162347,
      "grad_norm": 3.7833030223846436,
      "learning_rate": 0.00011970935513169845,
      "loss": 1.8695,
      "step": 450
    },
    {
      "epoch": 1.2537517053206002,
      "grad_norm": 3.167025566101074,
      "learning_rate": 0.00011789282470481381,
      "loss": 1.9089,
      "step": 460
    },
    {
      "epoch": 1.281036834924966,
      "grad_norm": 3.3085131645202637,
      "learning_rate": 0.00011607629427792916,
      "loss": 1.973,
      "step": 470
    },
    {
      "epoch": 1.3083219645293316,
      "grad_norm": 2.727565050125122,
      "learning_rate": 0.00011425976385104451,
      "loss": 1.9312,
      "step": 480
    },
    {
      "epoch": 1.3356070941336973,
      "grad_norm": 3.6507768630981445,
      "learning_rate": 0.00011244323342415985,
      "loss": 1.9134,
      "step": 490
    },
    {
      "epoch": 1.3628922237380627,
      "grad_norm": 3.3401336669921875,
      "learning_rate": 0.0001106267029972752,
      "loss": 1.9194,
      "step": 500
    },
    {
      "epoch": 1.3901773533424284,
      "grad_norm": 5.032230854034424,
      "learning_rate": 0.00010881017257039056,
      "loss": 1.9693,
      "step": 510
    },
    {
      "epoch": 1.4174624829467941,
      "grad_norm": 2.9683613777160645,
      "learning_rate": 0.00010699364214350591,
      "loss": 2.0062,
      "step": 520
    },
    {
      "epoch": 1.4447476125511596,
      "grad_norm": 3.003525972366333,
      "learning_rate": 0.00010517711171662125,
      "loss": 2.0547,
      "step": 530
    },
    {
      "epoch": 1.4720327421555253,
      "grad_norm": 2.058267116546631,
      "learning_rate": 0.0001033605812897366,
      "loss": 2.0285,
      "step": 540
    },
    {
      "epoch": 1.499317871759891,
      "grad_norm": 3.3515636920928955,
      "learning_rate": 0.00010154405086285195,
      "loss": 1.9091,
      "step": 550
    },
    {
      "epoch": 1.5266030013642564,
      "grad_norm": 4.009918689727783,
      "learning_rate": 9.97275204359673e-05,
      "loss": 1.9906,
      "step": 560
    },
    {
      "epoch": 1.553888130968622,
      "grad_norm": 4.757002353668213,
      "learning_rate": 9.791099000908266e-05,
      "loss": 1.9392,
      "step": 570
    },
    {
      "epoch": 1.5811732605729878,
      "grad_norm": 3.2071962356567383,
      "learning_rate": 9.6094459582198e-05,
      "loss": 1.916,
      "step": 580
    },
    {
      "epoch": 1.6084583901773533,
      "grad_norm": 3.334196090698242,
      "learning_rate": 9.427792915531336e-05,
      "loss": 1.8378,
      "step": 590
    },
    {
      "epoch": 1.635743519781719,
      "grad_norm": 3.1370527744293213,
      "learning_rate": 9.24613987284287e-05,
      "loss": 1.9141,
      "step": 600
    },
    {
      "epoch": 1.6630286493860846,
      "grad_norm": 3.833279609680176,
      "learning_rate": 9.064486830154405e-05,
      "loss": 1.9349,
      "step": 610
    },
    {
      "epoch": 1.69031377899045,
      "grad_norm": 2.548391103744507,
      "learning_rate": 8.88283378746594e-05,
      "loss": 1.941,
      "step": 620
    },
    {
      "epoch": 1.7175989085948158,
      "grad_norm": 2.8568718433380127,
      "learning_rate": 8.701180744777475e-05,
      "loss": 1.8968,
      "step": 630
    },
    {
      "epoch": 1.7448840381991815,
      "grad_norm": 4.072935581207275,
      "learning_rate": 8.519527702089011e-05,
      "loss": 1.8807,
      "step": 640
    },
    {
      "epoch": 1.772169167803547,
      "grad_norm": 4.090773105621338,
      "learning_rate": 8.337874659400545e-05,
      "loss": 1.9641,
      "step": 650
    },
    {
      "epoch": 1.7994542974079129,
      "grad_norm": 2.099712371826172,
      "learning_rate": 8.156221616712081e-05,
      "loss": 1.9715,
      "step": 660
    },
    {
      "epoch": 1.8267394270122783,
      "grad_norm": 3.0883798599243164,
      "learning_rate": 7.974568574023615e-05,
      "loss": 1.9301,
      "step": 670
    },
    {
      "epoch": 1.8540245566166438,
      "grad_norm": 3.3692257404327393,
      "learning_rate": 7.79291553133515e-05,
      "loss": 1.8597,
      "step": 680
    },
    {
      "epoch": 1.8813096862210097,
      "grad_norm": 2.1341965198516846,
      "learning_rate": 7.611262488646685e-05,
      "loss": 1.9616,
      "step": 690
    },
    {
      "epoch": 1.9085948158253752,
      "grad_norm": 2.993180990219116,
      "learning_rate": 7.42960944595822e-05,
      "loss": 1.8827,
      "step": 700
    },
    {
      "epoch": 1.9358799454297408,
      "grad_norm": 3.377380132675171,
      "learning_rate": 7.247956403269755e-05,
      "loss": 1.9487,
      "step": 710
    },
    {
      "epoch": 1.9631650750341065,
      "grad_norm": 4.793102264404297,
      "learning_rate": 7.06630336058129e-05,
      "loss": 1.8587,
      "step": 720
    },
    {
      "epoch": 1.990450204638472,
      "grad_norm": 2.968708038330078,
      "learning_rate": 6.884650317892825e-05,
      "loss": 1.9666,
      "step": 730
    },
    {
      "epoch": 2.0163710777626194,
      "grad_norm": 3.196730375289917,
      "learning_rate": 6.70299727520436e-05,
      "loss": 1.8998,
      "step": 740
    },
    {
      "epoch": 2.043656207366985,
      "grad_norm": 3.234043598175049,
      "learning_rate": 6.521344232515895e-05,
      "loss": 1.8121,
      "step": 750
    },
    {
      "epoch": 2.0709413369713507,
      "grad_norm": 2.809145212173462,
      "learning_rate": 6.33969118982743e-05,
      "loss": 1.8114,
      "step": 760
    },
    {
      "epoch": 2.098226466575716,
      "grad_norm": 3.658616542816162,
      "learning_rate": 6.158038147138965e-05,
      "loss": 1.8748,
      "step": 770
    },
    {
      "epoch": 2.1255115961800817,
      "grad_norm": 3.3069186210632324,
      "learning_rate": 5.9763851044505007e-05,
      "loss": 1.7469,
      "step": 780
    },
    {
      "epoch": 2.1527967257844476,
      "grad_norm": 3.012575149536133,
      "learning_rate": 5.794732061762035e-05,
      "loss": 1.8407,
      "step": 790
    },
    {
      "epoch": 2.180081855388813,
      "grad_norm": 4.176844120025635,
      "learning_rate": 5.613079019073569e-05,
      "loss": 1.7994,
      "step": 800
    },
    {
      "epoch": 2.2073669849931785,
      "grad_norm": 2.888291358947754,
      "learning_rate": 5.431425976385105e-05,
      "loss": 1.8811,
      "step": 810
    },
    {
      "epoch": 2.2346521145975444,
      "grad_norm": 3.021087884902954,
      "learning_rate": 5.249772933696639e-05,
      "loss": 1.7869,
      "step": 820
    },
    {
      "epoch": 2.26193724420191,
      "grad_norm": 3.645451545715332,
      "learning_rate": 5.068119891008175e-05,
      "loss": 1.7889,
      "step": 830
    },
    {
      "epoch": 2.2892223738062754,
      "grad_norm": 4.76478385925293,
      "learning_rate": 4.886466848319709e-05,
      "loss": 1.7432,
      "step": 840
    },
    {
      "epoch": 2.3165075034106413,
      "grad_norm": 3.720383644104004,
      "learning_rate": 4.704813805631245e-05,
      "loss": 1.8109,
      "step": 850
    },
    {
      "epoch": 2.3437926330150067,
      "grad_norm": 3.8135132789611816,
      "learning_rate": 4.52316076294278e-05,
      "loss": 1.7731,
      "step": 860
    },
    {
      "epoch": 2.3710777626193726,
      "grad_norm": 3.551636219024658,
      "learning_rate": 4.341507720254315e-05,
      "loss": 1.75,
      "step": 870
    },
    {
      "epoch": 2.398362892223738,
      "grad_norm": 3.3332579135894775,
      "learning_rate": 4.159854677565849e-05,
      "loss": 1.7328,
      "step": 880
    },
    {
      "epoch": 2.4256480218281036,
      "grad_norm": 3.7387943267822266,
      "learning_rate": 3.978201634877384e-05,
      "loss": 1.7854,
      "step": 890
    },
    {
      "epoch": 2.4529331514324695,
      "grad_norm": 2.9379725456237793,
      "learning_rate": 3.796548592188919e-05,
      "loss": 1.8563,
      "step": 900
    },
    {
      "epoch": 2.480218281036835,
      "grad_norm": 5.585768699645996,
      "learning_rate": 3.614895549500454e-05,
      "loss": 1.7895,
      "step": 910
    },
    {
      "epoch": 2.5075034106412004,
      "grad_norm": 5.375021457672119,
      "learning_rate": 3.433242506811989e-05,
      "loss": 1.8324,
      "step": 920
    },
    {
      "epoch": 2.5347885402455663,
      "grad_norm": 2.6601295471191406,
      "learning_rate": 3.251589464123524e-05,
      "loss": 1.7861,
      "step": 930
    },
    {
      "epoch": 2.562073669849932,
      "grad_norm": 4.999635219573975,
      "learning_rate": 3.069936421435059e-05,
      "loss": 1.8117,
      "step": 940
    },
    {
      "epoch": 2.5893587994542973,
      "grad_norm": 3.2804269790649414,
      "learning_rate": 2.888283378746594e-05,
      "loss": 1.8199,
      "step": 950
    },
    {
      "epoch": 2.616643929058663,
      "grad_norm": 3.1628479957580566,
      "learning_rate": 2.7066303360581292e-05,
      "loss": 1.7847,
      "step": 960
    },
    {
      "epoch": 2.6439290586630286,
      "grad_norm": 3.6007003784179688,
      "learning_rate": 2.5249772933696642e-05,
      "loss": 1.8494,
      "step": 970
    },
    {
      "epoch": 2.6712141882673945,
      "grad_norm": 3.360673189163208,
      "learning_rate": 2.343324250681199e-05,
      "loss": 1.7386,
      "step": 980
    },
    {
      "epoch": 2.69849931787176,
      "grad_norm": 2.8338675498962402,
      "learning_rate": 2.161671207992734e-05,
      "loss": 1.8064,
      "step": 990
    },
    {
      "epoch": 2.7257844474761255,
      "grad_norm": 3.8229498863220215,
      "learning_rate": 1.980018165304269e-05,
      "loss": 1.8481,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1101,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.630129551618867e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
